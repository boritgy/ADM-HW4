{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "fVU2eGDsrb2N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import itertools\n",
        "import collections\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qLOtMiDurb6r"
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle('/content/drive/MyDrive/ADM/Homework4/df.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_customers(df):\n",
        "  customers = df.drop_duplicates(subset='CustomerID')\n",
        "  customers = customers.drop(['TransactionID', 'CustomerID'], axis=1)\n",
        "  return customers\n",
        "\n",
        "# one hot encoding\n",
        "def encoder(df):\n",
        "  # CustomerDOB\n",
        "  df.CustomerDOB = pd.qcut(df.CustomerDOB.apply(lambda x: x.year), 3, labels = ['age_range_1' , 'age_range_2', 'age_range_3'])\n",
        "  # CustLocation\n",
        "  counts = df.CustLocation.value_counts()\n",
        "  min_frequency = 10\n",
        "  repl = counts[counts <= min_frequency].index\n",
        "  df.CustLocation = df.CustLocation.replace(repl, 'others')\n",
        "  # CustAccountBalance\n",
        "  df.CustAccountBalance = pd.qcut(df.CustAccountBalance, 3, labels = ['poor' , 'wealthy', 'rich'])\n",
        "  # TransactionTime\n",
        "  df.TransactionTime = pd.cut(df.TransactionTime.apply(lambda x: int(x.hour)), bins=[0, 13, 20, 24], labels = ['morning', 'afternoon', 'evening'])\n",
        "  # TransactionAmount (INR)\n",
        "  df['TransactionAmount (INR)'] = pd.qcut(df['TransactionAmount (INR)'], 3, labels = ['low', 'medium', 'high'])\n",
        "  # one hot encoding\n",
        "  df = pd.get_dummies(df)\n",
        "  return df\n",
        "\n",
        "from numpy.core.numeric import outer\n",
        "def compute_signature(l, hash_functions, D):\n",
        "  signature = []\n",
        "  for perm in hash_functions:\n",
        "    a,b = perm\n",
        "    min_hash = float('inf')\n",
        "    for el in l:\n",
        "      hash = (a*el + b) % D\n",
        "      if hash < min_hash:\n",
        "        min_hash = hash\n",
        "    signature.append(min_hash)\n",
        "  return signature\n",
        "\n",
        "\n",
        "def MinHash(df, seed):\n",
        "  N = 100   #number of hash functions\n",
        "  D = df.shape[1]   #number of indeces\n",
        "  minimum = 0    # inclusive\n",
        "  maximum = D    # exclusive\n",
        "  signature_matrix = np.zeros([len(df), N], dtype = np.int32)\n",
        "  p = 2943\n",
        "  np.random.seed(seed)\n",
        "  hash_functions = [(np.random.randint(minimum, maximum),np.random.randint(minimum, maximum)) for _ in range(N)]\n",
        "  df = scipy.sparse.lil_matrix(df)\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    signature_matrix[i] = compute_signature(df.rows[i], hash_functions, D)\n",
        "  return signature_matrix\n",
        "\n",
        "def create_buckets(sig_mat, b, r):\n",
        "    d, n = sig_mat.shape\n",
        "    assert(n==b*r)\n",
        "    buckets = collections.defaultdict(set)\n",
        "    bands = np.array_split(sig_mat, b, axis=1)\n",
        "    for i,band in tqdm(enumerate(bands)):\n",
        "        for j in range(d):\n",
        "            band_id = tuple(list(band[j,:])+[str(i)])\n",
        "            buckets[band_id].add(j)\n",
        "    return buckets\n",
        "\n",
        "# function that estimate the jaccard similarity\n",
        "def J_estimate(signature_matrix, user_index1, user_index2):\n",
        "  return (signature_matrix[user_index1, :] == signature_matrix[user_index2, :]).mean()\n",
        "\n",
        "# function that find similar customers to a given customer\n",
        "def find_similars(user_index, buckets):\n",
        "  similar_users = []\n",
        "  for value in buckets.values():\n",
        "    if user_index in value:\n",
        "      similar_users = similar_users + list(value)\n",
        "  similar_users = np.unique(np.array(similar_users))\n",
        "  return similar_users\n",
        "\n",
        "def compute_threshold(b, r):\n",
        "  return (1/b)**(1/r)"
      ],
      "metadata": {
        "id": "zJF3EtFvQC4U"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We choose values of b and r such that the threshold we obtain is very high, specifically around 80%. We see that for b=10 and r=10 the value is about 0.79"
      ],
      "metadata": {
        "id": "chE8NzxZ3nA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_threshold(b = 10, r = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft0OuDjs8zaK",
        "outputId": "7b0bb3ba-cb45-447e-e39a-08b8e314ed41"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7943282347242815"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = pd.read_pickle('/content/drive/MyDrive/ADM/Homework4/query_processed.pkl')"
      ],
      "metadata": {
        "id": "9kkQFhbg9Mht"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge the dataset with the query (the queries are the first 50 rows)\n",
        "query_plus_df = pd.concat([query, df])"
      ],
      "metadata": {
        "id": "d0saIA1F9UDb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess dataset\n",
        "customers = get_customers(query_plus_df)\n",
        "customers = encoder(customers)\n",
        "# LSH\n",
        "t0 = time.time()\n",
        "signature_matrix = MinHash(customers, seed=1234)\n",
        "# np.save('/content/drive/MyDrive/ADM/Homework4/signature_matrix.npy', signature_matrix)\n",
        "# signature_matrix = np.load('/content/drive/MyDrive/ADM/Homework4/signature_matrix.npy')\n",
        "buckets = create_buckets(signature_matrix, b = 10, r=10)\n",
        "\n",
        "# find similar customers to the query\n",
        "query_result = collections.defaultdict(list)\n",
        "for i in range(len(query)):\n",
        "  similars = find_similars(i, buckets)\n",
        "  query_result[i] = similars[similars > 49]  #remove queries themselves from results\n",
        "t1 = time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ielO2J6_86a9",
        "outputId": "268914c4-b3bd-497a-e4c9-d2fc5c62b087"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 884265/884265 [01:38<00:00, 9022.15it/s]\n",
            "10it [00:27,  2.79s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see if our LSH implementation works well we calculate the mean of the scores for each query element and then compute the mean of the results, this should be around 0.79."
      ],
      "metadata": {
        "id": "NILMR8glpTFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(np.array([np.mean(np.array(list(map(lambda x: J_estimate(signature_matrix, i, x), query_result[i])))) for i in range(50)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_WRy_8ppGjg",
        "outputId": "e46ed005-81ad-4818-967a-5827a57ac2da"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7540447180551679"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The running time is:"
      ],
      "metadata": {
        "id": "pvcFQXlhsU81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print((t1-t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr_bmUfMsUB_",
        "outputId": "a33d552d-ad30-452a-9306-b10e49d36686"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "159.63974261283875\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}